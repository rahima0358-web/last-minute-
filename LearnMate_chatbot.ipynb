{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "YafEBoCHkCYE",
      "metadata": {
        "id": "YafEBoCHkCYE"
      },
      "source": [
        "# LearnMate: AI-Powered RAG Chatbot for Students\n",
        "\n",
        "\n",
        "\n",
        "**Project Description:**\n",
        "LearnMate is a Retrieval-Augmented Generation (RAG) chatbot designed to act as an interactive study assistant. Instead of relying purely on an LLM's pre-trained knowledge, LearnMate dynamically searches uploaded course materials (like PDF textbooks) to find the exact information needed to answer a student's query. This minimizes hallucinations and ensures academic accuracy.\n",
        "\n",
        "This project demonstrates the end-to-end implementation of a RAG pipeline:\n",
        "1. **Document Loading & Processing**\n",
        "2. **Text Chunking**\n",
        "3. **Vector Embeddings & Storage (FAISS)**\n",
        "4. **Semantic Retrieval**\n",
        "5. **Answer Generation (using Llama 3 via Groq API)**\n",
        "\n",
        "Let's get started by setting up the environment dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kWsNdZ7wkCYG",
      "metadata": {
        "id": "kWsNdZ7wkCYG"
      },
      "outputs": [],
      "source": [
        "# Step 0: Install the tools we need\n",
        "\n",
        "%pip -q install langchain langchain-community langchain-text-splitters langchain-huggingface pypdf sentence-transformers faiss-cpu ipywidgets requests==2.32.4 groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LPhlb2jWkCYH",
      "metadata": {
        "id": "LPhlb2jWkCYH"
      },
      "outputs": [],
      "source": [
        "!unzip resources.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IwDStye5kCYI",
      "metadata": {
        "id": "IwDStye5kCYI"
      },
      "outputs": [],
      "source": [
        "import logging  \n",
        "from transformers.utils import logging as t_logging\n",
        "t_logging.set_verbosity_error()\n",
        "logging.getLogger(\"pypdf\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eTlzQKq0kCYI",
      "metadata": {
        "id": "eTlzQKq0kCYI"
      },
      "source": [
        "### Phase 1: Data Ingestion\n",
        "\n",
        "The first step in building the RAG pipeline is to ingest the knowledge base. For this implementation, I am using a sample HTML textbook provided in PDF format. I'll utilize LangChain's `PyPDFLoader` to parse and extract the text content sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UXiMdeeokCYI",
      "metadata": {
        "id": "UXiMdeeokCYI"
      },
      "outputs": [],
      "source": [
        "# PyPDFLoader is a tool from LangChain that reads PDF files page by page\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "file_path = \"HTML - Book.pdf\"\n",
        "loader = PyPDFLoader(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZcblZR7ykCYI",
      "metadata": {
        "id": "ZcblZR7ykCYI"
      },
      "outputs": [],
      "source": [
        "# Read the PDF — this loads all pages into memory\n",
        "document = loader.load()\n",
        "\n",
        "print(\"Number of pages in the document:\", len(document))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6C-o8-DDkCYJ",
      "metadata": {
        "id": "6C-o8-DDkCYJ"
      },
      "outputs": [],
      "source": [
        "# Let's peek at the first page to see what the loader extracted\n",
        "# You'll see the text content + some metadata (info about the file)\n",
        "\n",
        "print(\"First page of the document:\", document[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dg8UfCwDkCYJ",
      "metadata": {
        "id": "Dg8UfCwDkCYJ"
      },
      "source": [
        "### Phase 2: Document Chunking\n",
        "\n",
        "Feeding entire documents into an LLM is inefficient and often exceeds context token limits. To optimize retrieval, I will split the extracted text into smaller, overlapping chunks. I chose a chunk size of 500 characters with an overlap of 20 to preserve contextual continuity between adjacent segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j4E-S2PMkCYJ",
      "metadata": {
        "id": "j4E-S2PMkCYJ"
      },
      "outputs": [],
      "source": [
        "# Using a \"text splitter\" to cut the document into chunks\n",
        "# chunk_size=500 means each piece will be ~500 characters long (about 1 paragraph)\n",
        "# chunk_overlap=20 means pieces slightly overlap so we don't lose info at the edges\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "texts = text_splitter.split_documents(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vOZpMTvDkCYJ",
      "metadata": {
        "id": "vOZpMTvDkCYJ"
      },
      "outputs": [],
      "source": [
        "# A 5-page PDF might give us around 10-15 chunks\n",
        "\n",
        "print(\"Total number of chunks:\", len(texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Brd4WVK5kCYK",
      "metadata": {
        "id": "Brd4WVK5kCYK"
      },
      "outputs": [],
      "source": [
        "# Each chunk is a small piece of text from the PDF\n",
        "\n",
        "print(\"Example chunk:\")\n",
        "print(texts[5].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YqA0o3O7kCYK",
      "metadata": {
        "id": "YqA0o3O7kCYK"
      },
      "source": [
        "### Phase 3: Text Embeddings\n",
        "\n",
        "To enable semantic search—allowing the system to search by meaning rather than exact keyword matches—I need to convert the text chunks into dense mathematical vectors (embeddings). I'm utilizing the pre-trained `all-MiniLM-L6-v2` model from HuggingFace, which is lightweight yet highly effective for sentence-level semantic representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OegMuxJUkCYK",
      "metadata": {
        "id": "OegMuxJUkCYK"
      },
      "outputs": [],
      "source": [
        "# We'll use a free, pre-trained model to create our embeddings\n",
        "# This model was trained on millions of sentences so it understands meaning well\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B4QlMofukCYK",
      "metadata": {
        "id": "B4QlMofukCYK"
      },
      "outputs": [],
      "source": [
        "# Load the embedding model — this might take a minute the first time\n",
        "# \"all-MiniLM-L6-v2\" is a small but powerful model for understanding text meaning\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "print(\"Embeddings model ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qe9MhfVSkCYK",
      "metadata": {
        "id": "Qe9MhfVSkCYK"
      },
      "source": [
        "### Phase 4: Vector Database Initialization\n",
        "\n",
        "With the embeddings generated, I require a highly efficient database to store and query these vectors. I've integrated FAISS (Facebook AI Similarity Search), an open-source library that indexes the embeddings and allows for rapid similarity comparisons at scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7MfTjCapkCYK",
      "metadata": {
        "id": "7MfTjCapkCYK"
      },
      "outputs": [],
      "source": [
        "# FAISS is a fast vector database created by Facebook/Meta\n",
        "# It takes our text chunks + embedding model, converts everything to numbers,\n",
        "# and organizes them for fast searching\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vector_store = FAISS.from_documents(texts, embedding_model)\n",
        "\n",
        "print(\"Vector store ready! Total items stored:\", vector_store.index.ntotal)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MufqAG5TkCYK",
      "metadata": {
        "id": "MufqAG5TkCYK"
      },
      "source": [
        "### Phase 5: Semantic Retrieval Configuration\n",
        "\n",
        "With the FAISS vector store ready, I can instantiate the retrieval engine. I've configured it to execute a similarity search and return the top 3 most relevant text chunks (`k=3`) for any given user query. Below is a quick unit test to verify the retrieval accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pHiZwg3BkCYK",
      "metadata": {
        "id": "pHiZwg3BkCYK"
      },
      "outputs": [],
      "source": [
        "# Create a \"retriever\" that will find the top 3 most relevant chunks for any question\n",
        "# k=3 means: \"give me the 3 best matches\"\n",
        "\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GVwVWDTykCYK",
      "metadata": {
        "id": "GVwVWDTykCYK"
      },
      "outputs": [],
      "source": [
        "# Let's test it! Ask a question and see which chunks the system finds\n",
        "\n",
        "question = \"How to place the text in center?\"\n",
        "\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "# Print the top 3 chunks that matched our question\n",
        "for i, doc in enumerate(docs, start=1):\n",
        "    print(f\"--- Chunk {i} ---\")\n",
        "    print(doc.page_content[:400])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D1U_De15kCYK",
      "metadata": {
        "id": "D1U_De15kCYK"
      },
      "source": [
        "### Phase 6: LLM Integration and Answer Generation\n",
        "\n",
        "The retrieval system successfully isolates the raw context, but LearnMate needs to synthesize this data into conversational, human-readable answers.\n",
        "\n",
        "I am connecting to Groq's high-speed inference API utilizing the `llama-3.1-8b-instant` model. The system prompt is strictly engineered to force the LLM to answer *only* based on the provided context, preventing hallucinated information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kGc9MzhPkCYL",
      "metadata": {
        "id": "kGc9MzhPkCYL"
      },
      "outputs": [],
      "source": [
        "# Read API keys from file\n",
        "\n",
        "import random\n",
        "\n",
        "with open(\"api_keys.txt\", \"r\") as f:\n",
        "    api_keys = f.read().splitlines()\n",
        "\n",
        "random.shuffle(api_keys)\n",
        "\n",
        "api_key = api_keys[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_IjsaDyTkCYL",
      "metadata": {
        "id": "_IjsaDyTkCYL"
      },
      "outputs": [],
      "source": [
        "# Set up Groq — a free cloud AI service that will generate answers for us\n",
        "# Get your own free API key at: https://console.groq.com\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "GROQ_API_KEY = api_key\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "def build_prompt(context, question):\n",
        "    \"\"\"Create the instructions we send to the AI along with the context and question.\"\"\"\n",
        "    return (\n",
        "        f\"\"\"\n",
        "            You are a document-based Question Answering assistant helping students prepare for exams.\n",
        "\n",
        "            IMPORTANT:\n",
        "            The provided document context is the ONLY source of truth.\n",
        "            Answer strictly using information available in the document.\n",
        "            Do NOT use outside knowledge, assumptions, or prior training information.\n",
        "\n",
        "            Instructions:\n",
        "            1. Carefully read the entire document context before answering.\n",
        "            2. Extract the answer only from the provided context.\n",
        "            3. If relevant information appears in multiple places, combine them logically.\n",
        "            4. Do not invent, assume, or expand beyond the document.\n",
        "            5. If the answer is not clearly present in the context, respond exactly with:\n",
        "              Not found in document\n",
        "            6. Keep answers clear, simple, and concise (maximum 2–3 sentences).\n",
        "\n",
        "            Document Context:\n",
        "            {context}\n",
        "\n",
        "            Question:\n",
        "            {question}\n",
        "\n",
        "            Answer (based only on the document):\n",
        "\"\"\"\n",
        "    )\n",
        "\n",
        "def generate_answer(prompt):\n",
        "    \"\"\"Send our prompt to Groq and get an answer back.\"\"\"\n",
        "    try:\n",
        "        chat = client.chat.completions.create(\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return chat.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"Not found in document.\"\n",
        "\n",
        "print(\"Groq AI is ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bW5aGirEkCYL",
      "metadata": {
        "id": "bW5aGirEkCYL"
      },
      "source": [
        "### Phase 7: Interactive Chatbot User Interface\n",
        "\n",
        "To provide a seamless experience, I've constructed a front-end GUI directly within the Jupyter Notebook using `ipywidgets`. This allows end-users to organically query the textbook just like a standard chat application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tgoO6SFwkCYL",
      "metadata": {
        "id": "tgoO6SFwkCYL"
      },
      "outputs": [],
      "source": [
        "# Now try it yourself! Type any question about your PDF below.\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from google.colab import output\n",
        "\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "title = widgets.HTML(\"<h3>PDF Chatbot</h3><p style='color:gray'>Ask anything about the uploaded document</p>\")\n",
        "input_box = widgets.Text(\n",
        "    placeholder=\"Type your question here...\",\n",
        "    layout=widgets.Layout(width=\"70%\")\n",
        ")\n",
        "ask_button = widgets.Button(\n",
        "    description=\"Ask\",\n",
        "    button_style=\"primary\",\n",
        "    layout=widgets.Layout(width=\"100px\")\n",
        ")\n",
        "clear_button = widgets.Button(\n",
        "    description=\"Clear Chat\",\n",
        "    button_style=\"warning\",\n",
        "    layout=widgets.Layout(width=\"100px\")\n",
        ")\n",
        "chat_out = widgets.Output(\n",
        "    layout=widgets.Layout(\n",
        "        border=\"1px solid #ddd\",\n",
        "        padding=\"10px\",\n",
        "        height=\"350px\",\n",
        "        overflow_y=\"auto\"\n",
        "    )\n",
        ")\n",
        "status = widgets.HTML(\"\")\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "def handle_ask(_):\n",
        "    question = input_box.value.strip()\n",
        "    if not question:\n",
        "        return\n",
        "    input_box.value = \"\"\n",
        "    status.value = \"<i style='color:gray'>Thinking...</i>\"\n",
        "\n",
        "    docs = retriever.invoke(question)\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "    prompt = build_prompt(context, question)\n",
        "    answer = generate_answer(prompt)\n",
        "\n",
        "    chat_history.append((\"You\", question))\n",
        "    chat_history.append((\"Bot\", answer))\n",
        "    status.value = \"\"\n",
        "\n",
        "    with chat_out:\n",
        "        clear_output()\n",
        "        for role, text in chat_history:\n",
        "            if role == \"You\":\n",
        "                print(f\"You: {text}\\n\")\n",
        "            else:\n",
        "                print(f\"Bot: {text}\\n\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "def handle_clear(_):\n",
        "    chat_history.clear()\n",
        "    with chat_out:\n",
        "        clear_output()\n",
        "\n",
        "ask_button.on_click(handle_ask)\n",
        "clear_button.on_click(handle_clear)\n",
        "input_box.on_submit(handle_ask)\n",
        "\n",
        "buttons = widgets.HBox([ask_button, clear_button])\n",
        "display(title, widgets.HBox([input_box]), buttons, status, chat_out)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
